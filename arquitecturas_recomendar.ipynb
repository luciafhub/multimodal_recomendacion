{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Arquitecturas para tareas de recomendación"
      ],
      "metadata": {
        "id": "KtPiA58P2Qlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importación de librerías necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import numpy.core.numeric as _num\n",
        "sys.modules['numpy._core.numeric'] = _num  # Solución a problemas de importación con ciertas versiones de numpy en algunos entornos (como Kaggle)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error  # Métrica para calcular el error cuadrático medio (usado para RMSE)"
      ],
      "metadata": {
        "id": "b_v2DIf72VA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga de los datasets\n",
        "\n"
      ],
      "metadata": {
        "id": "jj-KB_jo2k8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se cargan los archivos preprocesados en formato pickle\n",
        "train_obj = pd.read_pickle(\"/kaggle/input/ny8010/train_v80.pkl\")\n",
        "val_obj   = pd.read_pickle(\"/kaggle/input/ny8010/val_v10.pkl\")\n",
        "test_obj  = pd.read_pickle(\"/kaggle/input/ny8010/test_v10.pkl\")\n",
        "\n",
        "# Si por alguna razón los objetos son listas, se convierten a DataFrame\n",
        "train_df = pd.DataFrame(train_obj) if isinstance(train_obj, list) else train_obj\n",
        "val_df   = pd.DataFrame(val_obj) if isinstance(val_obj, list) else val_obj\n",
        "test_df  = pd.DataFrame(test_obj) if isinstance(test_obj, list) else test_obj\n",
        "\n",
        "# Mostrar el número de ejemplos disponibles en cada partición\n",
        "print(\"Número de datos\")\n",
        "print(\"\\nTrain:\", len(train_df))\n",
        "print(\"Val:\", len(val_df))\n",
        "print(\"Test:\", len(test_df))\n",
        "print(\"\\nTotal:\", len(train_df) + len(val_df) + len(test_df))\n",
        "\n",
        "# Mostrar las columnas que contiene el DataFrame de entrenamiento\n",
        "print(\"\\nColumnas\")\n",
        "print(train_df.columns)\n",
        "\n",
        "# Asignación de alias más cortos para los datasets\n",
        "train = train_df\n",
        "val = val_df\n",
        "test = test_df"
      ],
      "metadata": {
        "id": "epxWe6fg2qqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Evaluación de baseline"
      ],
      "metadata": {
        "id": "PDw17rDm2cFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Asegurar que la columna 'rating' esté en formato float en los tres conjuntos\n",
        "for df in (train, val, test):\n",
        "    df['rating'] = df['rating'].astype(float)\n",
        "\n",
        "# Definición de la función para calcular el RMSE\n",
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "# Baseline: utilizar como predicción la media global de los ratings del conjunto de entrenamiento\n",
        "global_mean = train['rating'].mean()\n",
        "\n",
        "# Evaluación del baseline: se calcula el RMSE en train, val y test usando siempre la misma media\n",
        "results = []\n",
        "for name, df in [(\"Train\", train), (\"Val\", val), (\"Test\", test)]:\n",
        "    preds_global = np.full(len(df), global_mean)  # Array del mismo tamaño con la media como predicción constante\n",
        "    results.append({\n",
        "        'Baseline': 'Global Mean',\n",
        "        'Dataset':  name,\n",
        "        'RMSE':     rmse(df['rating'], preds_global)\n",
        "    })\n",
        "\n",
        "# Mostrar resultados en forma de tabla (filas = baseline, columnas = datasets)\n",
        "df_results = pd.DataFrame(results)\n",
        "print(\"\\nRMSE del baseline (media global):\")\n",
        "print(df_results.pivot(index='Baseline', columns='Dataset', values='RMSE').round(4))\n"
      ],
      "metadata": {
        "id": "-ft49NNq2LwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuración inicial"
      ],
      "metadata": {
        "id": "xXdRDO642JMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalación y carga de librerías necesarias\n",
        "import time                      # Para medir tiempos de ejecución\n",
        "import warnings                  # Para controlar la visualización de advertencias\n",
        "import torch                     # Framework principal de deep learning\n",
        "import torch.nn as nn            # Módulo para redes neuronales\n",
        "import torch.optim as optim      # Optimizadores como Adam\n",
        "import pandas as pd              # Carga y manejo de datos\n",
        "import numpy as np               # Operaciones numéricas con arrays\n",
        "import matplotlib.pyplot as plt  # Visualización de resultados\n",
        "from torch.utils.data import Dataset, DataLoader  # Dataset y cargadores en PyTorch\n",
        "from tqdm.auto import tqdm       # Barra de progreso para bucles\n",
        "\n",
        "# Configuración para ignorar warnings de tipo FutureWarning\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Configura el dispositivo: usa GPU si está disponible, sino CPU\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Habilita el uso de precisión mixta automática si hay GPU (más rápido y eficiente)\n",
        "use_amp = DEVICE == \"cuda\"\n",
        "\n",
        "# Definición de hiperparámetros para el entrenamiento de todos los modelos\n",
        "BATCH_SIZE = 128          # Tamaño de batch para los DataLoaders\n",
        "MAX_EPOCHS = 1000         # Número máximo de épocas de entrenamiento\n",
        "PATIENCE = 10             # Número de épocas sin mejora para activar early stopping\n",
        "EMB_DIM = 50              # Dimensión de los embeddings de usuario e ítem\n",
        "HIDDEN_DIM = 64           # Tamaño de la capa oculta en redes MLP\n",
        "LR_LIST = [1e-5, 1e-4, 5e-4, 1e-3]  # Lista de tasas de aprendizaje a probar\n",
        "WEIGHT_DECAY = 1e-6       # Regularización para evitar overfitting (L2)\n"
      ],
      "metadata": {
        "id": "py4Baxt_ykuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# clases dataset\n",
        "\n",
        "Las clases Dataset personalizadas definen cómo se organizan y extraen los datos desde un DataFrame para adaptarlos al entrenamiento de modelos en PyTorch. Cada clase está diseñada para un tipo específico de entrada: algunas usan identificadores numéricos de usuario y restaurante (FM_Dataset), otras trabajan con embeddings de texto o imagen generados previamente (CLIPTextDataset, CLIPImageDataset), y otras combinan distintas fuentes de información (MixedDataset, FullDataset). Estas clases son necesarias para que PyTorch pueda acceder a los datos de forma estructurada y eficiente.\n",
        "\n",
        "A partir de estas clases, se crean objetos llamados DataLoaders, que se encargan de cargar los datos por lotes (batches), barajarlos si es necesario, y alimentar automáticamente al modelo durante el entrenamiento y validación. Esto permite aprovechar mejor la GPU, reducir el consumo de memoria y facilitar el entrenamiento con datasets grandes. En resumen, Dataset define cómo se obtienen los datos, y DataLoader gestiona cómo se entregan al modelo en cada iteración."
      ],
      "metadata": {
        "id": "DDUOixqVys9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "# Definición de clases Dataset personalizadas\n",
        "# ============================================\n",
        "\n",
        "# Dataset clásico basado en codificación de usuario e ítem con sus IDs\n",
        "class FM_Dataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        # Convierte las columnas de IDs y rating a tensores de PyTorch\n",
        "        self.u = torch.tensor(df['user_id_new'].values, dtype=torch.long)\n",
        "        self.i = torch.tensor(df['restaurant_id_new'].values, dtype=torch.long)\n",
        "        self.r = torch.tensor(df['rating'].values, dtype=torch.float32)\n",
        "    def __len__(self): return len(self.r)\n",
        "    def __getitem__(self, idx): return self.u[idx], self.i[idx], self.r[idx]\n",
        "\n",
        "# Dataset para modelos que usan embeddings CLIP de texto como entrada\n",
        "class CLIPTextDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        # Combina los vectores de texto en una matriz y convierte a tensor\n",
        "        self.x = torch.from_numpy(np.vstack(df['text_emb'].values)).float()\n",
        "        self.y = torch.tensor(df['rating'].values, dtype=torch.float32)\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, idx): return self.x[idx], self.y[idx]\n",
        "\n",
        "# Dataset mixto que combina IDs con embeddings CLIP de texto\n",
        "class MixedDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.u = torch.tensor(df['user_id_new'].values, dtype=torch.long)\n",
        "        self.i = torch.tensor(df['restaurant_id_new'].values, dtype=torch.long)\n",
        "        self.x = torch.from_numpy(np.vstack(df['text_emb'].values)).float()\n",
        "        self.r = torch.tensor(df['rating'].values, dtype=torch.float32)\n",
        "    def __len__(self): return len(self.r)\n",
        "    def __getitem__(self, idx): return self.u[idx], self.i[idx], self.x[idx], self.r[idx]\n",
        "\n",
        "# Dataset para modelos que usan solo embeddings CLIP de imagen como entrada\n",
        "class CLIPImageDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.x = torch.from_numpy(np.stack(df['image_emb'].values)).float()\n",
        "        self.y = torch.tensor(df['rating'].astype(float).values, dtype=torch.float32)\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, idx): return self.x[idx], self.y[idx]\n",
        "\n",
        "# Dataset para modelos que combinan embeddings CLIP de imagen y texto\n",
        "class CLIPMixedDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        # Concatena los embeddings de imagen y texto horizontalmente (por columnas)\n",
        "        img_emb = np.stack(df['image_emb'].values)\n",
        "        txt_emb = np.stack(df['text_emb'].values)\n",
        "        self.x = torch.from_numpy(np.concatenate([img_emb, txt_emb], axis=1)).float()\n",
        "        self.y = torch.tensor(df['rating'].astype(float).values, dtype=torch.float32)\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, idx): return self.x[idx], self.y[idx]\n",
        "\n",
        "# Dataset más completo: incluye IDs, embeddings de imagen y de texto\n",
        "class FullDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.u = torch.tensor(df['user_enc'].values, dtype=torch.long)\n",
        "        self.i = torch.tensor(df['item_enc'].values, dtype=torch.long)\n",
        "        img_emb = np.stack(df['image_emb'].values)\n",
        "        txt_emb = np.stack(df['text_emb'].values)\n",
        "        self.x = torch.from_numpy(np.concatenate([img_emb, txt_emb], axis=1)).float()\n",
        "        self.y = torch.tensor(df['rating'].astype(float).values, dtype=torch.float32)\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, idx): return self.u[idx], self.i[idx], self.x[idx], self.y[idx]\n"
      ],
      "metadata": {
        "id": "GkVNv-O7yv3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aquitecutras\n",
        "\n",
        "## Tabla resumen de arquitecturas de recomendación\n",
        "\n",
        "| Nº | Nombre del Modelo           | Entradas utilizadas                                  | Tipo de arquitectura         | Comentario breve |\n",
        "|----|-----------------------------|------------------------------------------------------|------------------------------|------------------|\n",
        "| 1  | MatrixFactorization         | ID de usuario + ID de restaurante                    | Producto punto               | Factorización clásica sin red neuronal |\n",
        "| 2  | NeuralRecommender           | ID de usuario + ID de restaurante                    | MLP                          | Recomendador neuronal que concatena embeddings |\n",
        "| 3  | CLIPTextRegressor           | Embedding CLIP del texto                             | MLP                          | Solo utiliza el contenido textual |\n",
        "| 4  | MixedModel                  | ID de usuario + ID de restaurante + texto emb       | MLP                          | Mezcla codificación tradicional con texto |\n",
        "| 5  | CLIPImageRegressor          | Embedding CLIP de imagen                             | MLP                          | Solo utiliza el contenido visual |\n",
        "| 6  | CLIPMixedRegressor          | Embedding CLIP de imagen + texto                    | MLP                          | Fusiona ambos tipos de contenido multimodal |\n",
        "| 7  | FullModel                   | ID de usuario + ID de restaurante + img + texto     | MLP                          | Modelo más completo, combina IDs y multimodalidad |\n"
      ],
      "metadata": {
        "id": "AwJ4mdEc1MQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Definición de modelos de recomendación\n",
        "# ============================================\n",
        "\n",
        "# Arquitectura 1: Factorización Matricial\n",
        "# Aprende un vector de características (embedding) por usuario y por ítem.\n",
        "# La predicción del rating se obtiene como el producto punto entre ambos embeddings.\n",
        "class MatrixFactorization(nn.Module):\n",
        "    def __init__(self, n_users, n_items):\n",
        "        super().__init__()\n",
        "        # Embeddings para usuarios e ítems\n",
        "        self.Eu = nn.Embedding(n_users, EMB_DIM)\n",
        "        self.Ei = nn.Embedding(n_items, EMB_DIM)\n",
        "        # Inicialización normal de pesos\n",
        "        nn.init.normal_(self.Eu.weight, mean=1.0, std=0.01)\n",
        "        nn.init.normal_(self.Ei.weight, mean=1.0, std=0.01)\n",
        "\n",
        "    def forward(self, u, i):\n",
        "        # Producto punto entre los embeddings del usuario y del ítem\n",
        "        return (self.Eu(u) * self.Ei(i)).sum(dim=1)\n",
        "\n",
        "# Arquitectura 2: Recomendador neuronal (MLP)\n",
        "# Similar a la anterior, pero en vez de usar producto punto, combina los embeddings con una red neuronal.\n",
        "class NeuralRecommender(nn.Module):\n",
        "    def __init__(self, n_users, n_items):\n",
        "        super().__init__()\n",
        "        # Embeddings para usuarios e ítems\n",
        "        self.user_emb = nn.Embedding(n_users, EMB_DIM)\n",
        "        self.item_emb = nn.Embedding(n_items, EMB_DIM)\n",
        "        # Inicialización normal de pesos\n",
        "        nn.init.normal_(self.user_emb.weight, mean=1.0, std=0.01)\n",
        "        nn.init.normal_(self.item_emb.weight, mean=1.0, std=0.01)\n",
        "        # Red neuronal para predecir el rating a partir de los embeddings concatenados\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(2 * EMB_DIM, HIDDEN_DIM),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(HIDDEN_DIM, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, u, i):\n",
        "        # Concatenación de los embeddings de usuario e ítem\n",
        "        x = torch.cat([self.user_emb(u), self.item_emb(i)], dim=1)\n",
        "        # Predicción del rating\n",
        "        return self.mlp(x).squeeze(1)\n",
        "\n",
        "# Arquitecturas 3, 5 y 6: Regresores para embeddings (texto, imagen o combinación)\n",
        "# Red neuronal que toma directamente un vector (embedding CLIP) como entrada y predice un rating.\n",
        "class CLIPRegressor(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        # Red MLP con 3 capas para regresión\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mlp(x).squeeze(1)\n",
        "\n",
        "# Arquitectura 4: Modelo mixto (IDs + embeddings de texto)\n",
        "# Combina los embeddings de usuario e ítem con los vectores CLIP de texto.\n",
        "class MixedModel(nn.Module):\n",
        "    def __init__(self, n_users, n_items, clip_dim):\n",
        "        super().__init__()\n",
        "        self.user_emb = nn.Embedding(n_users, EMB_DIM)\n",
        "        self.item_emb = nn.Embedding(n_items, EMB_DIM)\n",
        "        nn.init.normal_(self.user_emb.weight, mean=1.0, std=0.01)\n",
        "        nn.init.normal_(self.item_emb.weight, mean=1.0, std=0.01)\n",
        "        # MLP que procesa los embeddings junto con el texto\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(clip_dim + 2 * EMB_DIM, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, u, i, x):\n",
        "        # Concatenación de embeddings de usuario, ítem y texto CLIP\n",
        "        x = torch.cat([self.user_emb(u), self.item_emb(i), x], dim=1)\n",
        "        return self.mlp(x).squeeze(1)\n",
        "\n",
        "# Arquitectura 7: Modelo completo (IDs + imagen + texto)\n",
        "# Utiliza toda la información disponible: IDs, embeddings CLIP de imagen y de texto.\n",
        "class FullModel(nn.Module):\n",
        "    def __init__(self, n_users, n_items, clip_dim=1024, emb_dim=50):\n",
        "        super().__init__()\n",
        "        self.user_emb = nn.Embedding(n_users, emb_dim)\n",
        "        self.item_emb = nn.Embedding(n_items, emb_dim)\n",
        "        nn.init.normal_(self.user_emb.weight, mean=1.0, std=0.01)\n",
        "        nn.init.normal_(self.item_emb.weight, mean=1.0, std=0.01)\n",
        "        # MLP que combina embeddings de usuario, ítem y embeddings CLIP (texto + imagen)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(clip_dim + 2 * emb_dim, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, u, i, x):\n",
        "        # Concatenación de toda la información (IDs + texto + imagen)\n",
        "        concat = torch.cat([self.user_emb(u), self.item_emb(i), x], dim=1)\n",
        "        return self.mlp(concat).squeeze(1)\n"
      ],
      "metadata": {
        "id": "VlvnNu-k1Lra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenar y resultados"
      ],
      "metadata": {
        "id": "7VWoHccrykar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# Funciones auxiliares: entrenamiento y evaluación\n",
        "# ============================================\n",
        "\n",
        "# Función para calcular el RMSE en un conjunto de datos (validación o test)\n",
        "def compute_rmse(model, loader, criterion):\n",
        "    model.eval()  # Se pone el modelo en modo evaluación (desactiva dropout, batchnorm, etc.)\n",
        "    total, n = 0.0, 0\n",
        "    with torch.no_grad():  # No se calculan gradientes\n",
        "        for batch in loader:\n",
        "            batch = [b.to(DEVICE) for b in batch]  # Se envían los tensores al dispositivo\n",
        "            pred, y = forward_batch(model, batch)  # Se obtienen predicciones y etiquetas reales\n",
        "            loss = criterion(pred, y)  # Se calcula el error cuadrático medio\n",
        "            total += loss.item() * y.size(0)  # Se acumula el error total\n",
        "            n += y.size(0)  # Se acumulan las muestras procesadas\n",
        "    return np.sqrt(total / n)  # Se devuelve la raíz del error cuadrático medio (RMSE)\n",
        "\n",
        "# Función generalizada para hacer forward pass compatible con distintos tipos de entrada\n",
        "def forward_batch(model, batch):\n",
        "    if len(batch) == 2:\n",
        "        # Casos con solo features (x) y etiquetas (y)\n",
        "        x, y = batch\n",
        "        pred = model(x)\n",
        "    elif len(batch) == 3:\n",
        "        # Casos con user ID, item ID y etiquetas (factorización matricial o MLP con IDs)\n",
        "        u, i, y = batch\n",
        "        pred = model(u, i)\n",
        "    else:\n",
        "        # Casos con user ID, item ID, embeddings (texto/imagen) y etiquetas\n",
        "        u, i, x, y = batch\n",
        "        pred = model(u, i, x)\n",
        "    return pred, y\n",
        "\n",
        "# Función principal para entrenar un modelo con validación y early stopping\n",
        "def train_model(name, model_class, dataset_class, train_df, val_df, test_df, extra_args={}):\n",
        "    print(f\"\\n===> Entrenando {name}\")\n",
        "\n",
        "    # Creación de DataLoaders para entrenamiento, validación y test\n",
        "    train_loader = DataLoader(dataset_class(train_df), batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader   = DataLoader(dataset_class(val_df), batch_size=BATCH_SIZE)\n",
        "    test_loader  = DataLoader(dataset_class(test_df), batch_size=BATCH_SIZE)\n",
        "\n",
        "    criterion = nn.MSELoss()  # Se utiliza el error cuadrático medio como función de pérdida\n",
        "    results = []  # Lista para almacenar resultados por tasa de aprendizaje\n",
        "\n",
        "    # Se entrena el modelo con distintas tasas de aprendizaje\n",
        "    for lr in LR_LIST:\n",
        "        model = model_class(**extra_args).to(DEVICE)  # Se inicializa el modelo\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)  # Optimizador Adam\n",
        "        scaler = torch.cuda.amp.GradScaler() if use_amp else None  # Escalado automático de precisión (AMP)\n",
        "\n",
        "        # Variables para early stopping\n",
        "        best_val, best_state, best_epoch = float('inf'), None, 0\n",
        "        train_hist, val_hist = [], []\n",
        "        no_improve = 0\n",
        "\n",
        "        # Entrenamiento por épocas\n",
        "        for epoch in tqdm(range(1, MAX_EPOCHS + 1), desc=f\"{name} lr={lr}\"):\n",
        "            model.train()\n",
        "            total_loss, count = 0.0, 0\n",
        "\n",
        "            for batch in train_loader:\n",
        "                batch = [b.to(DEVICE) for b in batch]\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                if use_amp:\n",
        "                    # Entrenamiento con AMP (si se usa)\n",
        "                    with torch.cuda.amp.autocast():\n",
        "                        pred, y = forward_batch(model, batch)\n",
        "                        loss = criterion(pred, y)\n",
        "                    scaler.scale(loss).backward()\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                else:\n",
        "                    # Entrenamiento clásico sin AMP\n",
        "                    pred, y = forward_batch(model, batch)\n",
        "                    loss = criterion(pred, y)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                total_loss += loss.item() * y.size(0)\n",
        "                count += y.size(0)\n",
        "\n",
        "            # Evaluación tras cada época\n",
        "            train_rmse = np.sqrt(total_loss / count)\n",
        "            val_rmse = compute_rmse(model, val_loader, criterion)\n",
        "            train_hist.append(train_rmse)\n",
        "            val_hist.append(val_rmse)\n",
        "\n",
        "            # Verificación para early stopping\n",
        "            if val_rmse < best_val:\n",
        "                best_val, best_state, best_epoch = val_rmse, model.state_dict(), epoch\n",
        "                no_improve = 0\n",
        "            else:\n",
        "                no_improve += 1\n",
        "                if no_improve >= PATIENCE:  # No mejora tras 'PATIENCE' épocas\n",
        "                    break\n",
        "\n",
        "        # Se carga el mejor modelo y se evalúa en test\n",
        "        model.load_state_dict(best_state)\n",
        "        test_rmse = compute_rmse(model, test_loader, criterion)\n",
        "\n",
        "        # Se guarda el resultado para esta tasa de aprendizaje\n",
        "        results.append({\n",
        "            'Arquitectura': name,\n",
        "            'learning_rate': lr,\n",
        "            'train_rmse': train_hist[best_epoch - 1],\n",
        "            'val_rmse': best_val,\n",
        "            'test_rmse': test_rmse,\n",
        "            'epochs': best_epoch\n",
        "        })\n",
        "\n",
        "        # Guardado de la curva de entrenamiento\n",
        "        plt.figure()\n",
        "        plt.plot(train_hist, label='Train RMSE')\n",
        "        plt.plot(val_hist, label='Val RMSE')\n",
        "        plt.axvline(best_epoch, linestyle='--', color='gray', label='Best Epoch')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('RMSE')\n",
        "        plt.title(f'{name} @ lr={lr}')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f'curve_{name.replace(\"-\", \"_\")}_lr_{lr}.pdf')\n",
        "        plt.close()\n",
        "\n",
        "    # Exportación de resultados a Excel\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_excel(f\"{name}.xlsx\", index=False)\n",
        "    print(f\"✅ Resultados guardados en {name}.xlsx\")\n",
        "\n",
        "# ============================================\n",
        "# Lista de arquitecturas a entrenar\n",
        "# ============================================\n",
        "\n",
        "# Cada tupla incluye:\n",
        "# - Nombre del modelo\n",
        "# - Clase del modelo (PyTorch)\n",
        "# - Dataset correspondiente\n",
        "# - Parámetros adicionales requeridos (como nº de usuarios, ítems o dimensión de embeddings)\n",
        "ARQUITECTURAS = [\n",
        "    (\"1-FM\", MatrixFactorization, FM_Dataset, {'n_users': train['user_id_new'].nunique(), 'n_items': train['restaurant_id_new'].nunique()}),\n",
        "    (\"2-RN\", NeuralRecommender, FM_Dataset, {'n_users': train['user_id_new'].nunique(), 'n_items': train['restaurant_id_new'].nunique()}),\n",
        "    (\"3-CLIP-TEXT\", CLIPRegressor, CLIPTextDataset, {'input_dim': len(train['text_emb'].iloc[0])}),\n",
        "    (\"4-MIX-TXTEN\", MixedModel, MixedDataset, {'n_users': train['user_id_new'].nunique(), 'n_items': train['restaurant_id_new'].nunique(), 'clip_dim': len(train['text_emb'].iloc[0])}),\n",
        "    (\"5-CLIP-IMG\", CLIPRegressor, CLIPImageDataset, {'input_dim': len(train['image_emb'].iloc[0])}),\n",
        "    (\"6-TXT+IMG\", CLIPRegressor, CLIPMixedDataset, {'input_dim': len(train['image_emb'].iloc[0]) + len(train['text_emb'].iloc[0])}),\n",
        "    (\"7-FULL\", FullModel, FullDataset, {'n_users': train['user_enc'].nunique(), 'n_items': train['item_enc'].nunique()})\n",
        "]\n",
        "\n",
        "# Entrenamiento de todas las arquitecturas con evaluación y guardado\n",
        "start_all = time.time()\n",
        "for name, model_cls, dataset_cls, extra_args in ARQUITECTURAS:\n",
        "    train_model(name, model_cls, dataset_cls, train, val, test, extra_args)\n",
        "print(f\"\\n⏱️ Tiempo total: {(time.time() - start_all)/60:.2f} minutos\")\n"
      ],
      "metadata": {
        "id": "i7Lsu33x16S0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}