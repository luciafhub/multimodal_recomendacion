{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ffe857",
   "metadata": {},
   "source": [
    "# Generación de Embeddings Multimodales con CLIP\n",
    "\n",
    "Autor: Lucía Fernández Rodríguez\n",
    "Fecha: 2025\n",
    "\n",
    "Este notebook genera representaciones vectoriales (embeddings) multimodales para reseñas de restaurantes utilizando el modelo CLIP de OpenAI. Primero codifica los textos de las reseñas y, posteriormente, descarga (verificando que las imágenes siguen disponibles con su enlace) y codifica las imágenes asociadas. El resultado es un conjunto de datos enriquecido con embeddings que pueden utilizarse en tareas de recomendación o análisis posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db911031",
   "metadata": {},
   "source": [
    "### Instalación de la librería CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a71bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4a826a",
   "metadata": {},
   "source": [
    "### Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e87e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import clip\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import os\n",
    "import json, ast, requests\n",
    "from PIL import Image\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9ac817",
   "metadata": {},
   "source": [
    "## Generación de Embeddings de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170e9fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta del archivo de entrada (ciudad a escoger)\n",
    "input_pkl = \"/kaggle/input/gijon\"\n",
    "\n",
    "# Configuración del modelo y dispositivo\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Tamaño del batch para procesar texto\n",
    "BATCH_TEXT = 32\n",
    "\n",
    "# Ruta del archivo que se genera con el embedding del texto\n",
    "output_pkl = \"/kaggle/working/gijon_textemb.pkl\"\n",
    "\n",
    "# Cargar el dataset de entrada\n",
    "df = pd.read_pickle(input_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5846fe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamiento de textos y generación de embeddings en lotes\n",
    "texts = df[\"text\"].fillna(\"\").astype(str).tolist()\n",
    "text_emb_en = []\n",
    "\n",
    "for i in tqdm(range(0, len(texts), BATCH_TEXT), desc=\"Embedding de textos\"):\n",
    "    batch = texts[i : i + BATCH_TEXT]\n",
    "    toks = clip.tokenize(batch, truncate=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        embs = model.encode_text(toks).cpu().numpy()\n",
    "    text_emb_en.append(embs)\n",
    "\n",
    "# Concatenación de todos los embeddings\n",
    "text_emb_en = np.vstack(text_emb_en)\n",
    "\n",
    "# Añadir los embeddings al DataFrame y guardar\n",
    "df[\"text_emb\"] = list(text_emb_en)\n",
    "df.to_pickle(output_pkl)\n",
    "print(f\"Texto embebido añadido y guardado en {output_pkl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc605da",
   "metadata": {},
   "source": [
    "## Generación de Embeddings de Imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d65b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta del archivo de entrada (con embeddings de texto genrados en el bloque anterior)\n",
    "input_pkl = \"/kaggle/input/gijon_textemb.pkl\"\n",
    "\n",
    "# Ruta del archivo que se genera como salida\n",
    "output_pkl = \"/kaggle/working/gijon_emb.pkl\"\n",
    "\n",
    "# Configuración de modelo y batch de imágenes\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "BATCH_IMG = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec952b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de columnas relevantes\n",
    "cols_to_keep = ['images', 'rating', 'restaurantId', 'reviewId', 'text', 'text_emb', 'userId']\n",
    "\n",
    "# Carga del dataset y filtrado de columnas\n",
    "df = pd.read_pickle(input_pkl)\n",
    "df = df[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848e3f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función auxiliar para extraer una URL desde una entrada de imagen\n",
    "def extract_url(item):\n",
    "    \"\"\"\n",
    "    Extrae la URL 'image_url_lowres' desde un objeto que puede ser:\n",
    "    - Un diccionario\n",
    "    - Una cadena con formato de diccionario o JSON\n",
    "    \"\"\"\n",
    "    if isinstance(item, dict):\n",
    "        return item.get(\"image_url_lowres\", None)\n",
    "    elif isinstance(item, str):\n",
    "        try:\n",
    "            return ast.literal_eval(item).get(\"image_url_lowres\", None)\n",
    "        except:\n",
    "            try:\n",
    "                return json.loads(item.replace(\"'\", '\"')).get(\"image_url_lowres\", None)\n",
    "            except:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "# Extracción de la primera imagen válida por reseña\n",
    "urls = []\n",
    "idxs = []\n",
    "for idx, imgs in enumerate(df[\"images\"]):\n",
    "    if isinstance(imgs, list) and len(imgs) > 0:\n",
    "        url = extract_url(imgs[0])\n",
    "        if url:\n",
    "            urls.append(url)\n",
    "            idxs.append(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71e6a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_url(item):\n",
    "    if isinstance(item, dict):\n",
    "        return item.get(\"image_url_lowres\", None)\n",
    "    elif isinstance(item, str):\n",
    "        try:\n",
    "            return ast.literal_eval(item).get(\"image_url_lowres\", None)\n",
    "        except:\n",
    "            try:\n",
    "                return json.loads(item.replace(\"'\", '\"')).get(\"image_url_lowres\", None)\n",
    "            except:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "urls, idxs = [], []\n",
    "for idx, imgs in enumerate(df[\"images\"]):\n",
    "    if isinstance(imgs, list) and len(imgs) > 0:\n",
    "        url = extract_url(imgs[0])\n",
    "        if url:\n",
    "            urls.append(url)\n",
    "            idxs.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db01b122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesamiento por lotes de imágenes\n",
    "image_embs = []\n",
    "sel_urls = []\n",
    "valid_idxs = []\n",
    "\n",
    "for i in tqdm(range(0, len(urls), BATCH_IMG), desc=\"Procesando lotes de imágenes\"):\n",
    "    batch_urls = urls[i:i + BATCH_IMG]\n",
    "    batch_imgs = []\n",
    "    batch_valid = []\n",
    "\n",
    "    # Descargar imágenes válidas y convertirlas a tensores\n",
    "    for j, url in enumerate(batch_urls):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=5)\n",
    "            if r.status_code == 200:\n",
    "                img = Image.open(BytesIO(r.content)).convert(\"RGB\")\n",
    "                img_tensor = preprocess(img)\n",
    "                batch_imgs.append(img_tensor)\n",
    "                batch_valid.append(j)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if not batch_imgs:\n",
    "        continue\n",
    "\n",
    "    # Codificar imágenes con CLIP\n",
    "    batch_tensor = torch.stack(batch_imgs).to(device)\n",
    "    with torch.no_grad():\n",
    "        batch_embs = model.encode_image(batch_tensor).cpu().numpy()\n",
    "\n",
    "    # Guardar embeddings y sus índices\n",
    "    for j in range(len(batch_embs)):\n",
    "        orig_idx = idxs[i + batch_valid[j]]\n",
    "        image_embs.append(batch_embs[j])\n",
    "        sel_urls.append(urls[i + batch_valid[j]])\n",
    "        valid_idxs.append(orig_idx)\n",
    "\n",
    "# Crear nuevo DataFrame solo con reseñas con imagen válida\n",
    "df = df.iloc[valid_idxs].copy()\n",
    "df[\"image_emb\"] = image_embs\n",
    "df[\"sel_image_url\"] = sel_urls\n",
    "df = df[cols_to_keep + [\"image_emb\", \"sel_image_url\"]]\n",
    "\n",
    "# Guardar resultado final\n",
    "df.to_pickle(output_pkl)\n",
    "print(f\"Embeddings guardados en {output_pkl} ({len(df)} reseñas con imagen válida)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a477e3a",
   "metadata": {},
   "source": [
    "## Visualizar archivo final generado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e12933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver las primeras líneas\n",
    "print(output_pkl.head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
